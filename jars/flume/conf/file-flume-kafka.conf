# 组件定义
a1.sources=r1
a1.channels=c1 c2

# configure source
a1.sources.r1.type = TAILDIR
# 记录文件索引节点的目录，实现断点续传
a1.sources.r1.positionFile = /opt/module/flume/test/log_position.json
a1.sources.r1.channels = c1 c2 
# 监控的目录
a1.sources.r1.filegroups = f1 
a1.sources.r1.filegroups.f1 = /tmp/logs/app.+
a1.sources.r1.fileHeader = true

#interceptor 拦截器
a1.sources.r1.interceptors =  i1 i2
a1.sources.r1.interceptors.i1.type = com.xzt.flume.interceptor.LogETLInterceptor$Builder
a1.sources.r1.interceptors.i2.type = com.xzt.flume.interceptor.LogTypeInterceptor$Builder

#channels 选择器
a1.sources.r1.selector.type = multiplexing
a1.sources.r1.selector.header = topic
a1.sources.r1.selector.mapping.topic_start = c1
a1.sources.r1.selector.mapping.topic_event = c2

# configure channel
a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
# kafka 集群
a1.channels.c1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
# kafka topic
a1.channels.c1.kafka.topic = topic_start
# 默认是true , 会在解析完json后在前面加一个topic主题的前缀 ，这边我们不需要，否则到HIVE后还要将前缀去掉
a1.channels.c1.parseAsFlumeEvent = false
# kafka 消费者组
a1.channels.c1.kafka.consumer.group.id = flume-consumer

a1.channels.c2.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c2.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.channels.c2.kafka.topic = topic_event
a1.channels.c2.parseAsFlumeEvent = false
a1.channels.c2.kafka.consumer.group.id = flume-consumer






